{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereâ€™s an example of my script in which I make multilayer perceptron on some financial data. The data has 53 independent variables (X) and one independent variable (y). It is time series data and X exhibits plenty of time-series related information. The goal is to fit MP as good as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Import packages:\n",
    "import numpy as np\n",
    "import sys\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from keras.models import Sequential\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96674\n"
     ]
    }
   ],
   "source": [
    "### Load data and count the number of samples.\n",
    "input_data_all = np.load('train_sample.npy')\n",
    "output_data_all= np.load('train_target.npy')\n",
    "n_samples= input_data_all.shape[0]\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### shuffle 2 data series together randomly.\n",
    "def shuffle_together(input_1, input_2):\n",
    "    if input_1.shape[0]!= input_2.shape[0]:\n",
    "        print (\"Problem, y and x array are not of the same shape.\")\n",
    "        return None\n",
    "    c= np.arange(input_1.shape[0])\n",
    "    np.random.shuffle(c)\n",
    "    return input_1[c], input_2[c], c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87007, 53)\n",
      "(87007,)\n"
     ]
    }
   ],
   "source": [
    "### splitting on train and validation input (note that it is splitted randomly).\n",
    "\n",
    "input_data_all, output_data_all, _ = shuffle_together(input_data_all, output_data_all)\n",
    "n_validation_samples= int(0.1*n_samples)\n",
    "validation_input= input_data_all[-1*n_validation_samples:]\n",
    "validation_output= output_data_all[-1*n_validation_samples:]\n",
    "n_train_samples= n_samples-n_validation_samples\n",
    "train_input= input_data_all[:n_train_samples]\n",
    "train_output= output_data_all[:n_train_samples]\n",
    "print(train_input.shape)\n",
    "print(train_output.shape)\n",
    "### Now we define matrix x and vector y in theano.\n",
    "x=T.matrix('x', dtype= theano.config.floatX)\n",
    "y=T.vector('y', dtype= theano.config.floatX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = keras.models.Sequential()\n",
    "### First layer.  Linear activation layer.\n",
    "model.add(keras.layers.Dense(input_data_all.shape[1], input_shape=(train_input.shape[1],),activation='linear',\n",
    "                bias_initializer=keras.initializers.RandomUniform(minval=-1, maxval=1, seed=None),\n",
    "                kernel_initializer=keras.initializers.RandomUniform(minval=-1, maxval=1, seed=None)))\n",
    "\n",
    "# add hidden layer(s) with relu and tanh activation functions.\n",
    "model.add(keras.layers.Dense(units=40,kernel_initializer=keras.initializers.RandomUniform(minval=-1, maxval=1, seed=None),\n",
    "        bias_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "        activation='relu')\n",
    "    )          \n",
    "model.add(keras.layers.Dense(units=25,kernel_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "        bias_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "        activation='tanh')\n",
    "    )          \n",
    "model.add(keras.layers.Dense(units=15,kernel_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "        bias_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "        activation='tanh')\n",
    "    )          \n",
    "model.add(keras.layers.Dense(units=8,kernel_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "        bias_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "        activation='tanh')\n",
    "    )          \n",
    "model.add(keras.layers.Dense(units=4,kernel_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "        bias_initializer=keras.initializers.RandomUniform(minval=-1, maxval=1, seed=None),\n",
    "        activation='relu')\n",
    "    )          \n",
    "### Final layer is again linear.\n",
    "model.add(keras.layers.Dense(units=1,kernel_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "        bias_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "        activation='linear')\n",
    "    )          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define SGD optimizer\n",
    "sgd_optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-012)\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss='mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87006 samples, validate on 9668 samples\n",
      "Epoch 1/100000\n",
      "87006/87006 [==============================] - 9s 103us/step - loss: 1.2347\n",
      "Epoch 2/100000\n",
      "87006/87006 [==============================] - 12s 141us/step - loss: 0.0443ETA: 2s - los - ETA: 2s  - ETA: 1s  - ETA: 0s -\n",
      "Epoch 3/100000\n",
      "87006/87006 [==============================] - 14s 163us/step - loss: 0.04173s  - ETA:\n",
      "Epoch 4/100000\n",
      "87006/87006 [==============================] - 7s 79us/step - loss: 0.0417\n",
      "Epoch 5/100000\n",
      "87006/87006 [==============================] - 4s 45us/step - loss: 0.0417\n",
      "Epoch 6/100000\n",
      "87006/87006 [==============================] - 6s 74us/step - loss: 0.0417: 1s - lo\n",
      "Epoch 7/100000\n",
      "87006/87006 [==============================] - 8s 87us/step - loss: 0.0417\n",
      "Epoch 8/100000\n",
      "59100/87006 [===================>..........] - ETA: 2s - loss: 0.0418- ETA:  - E - ETA: 2s - loss: 0.04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.100987). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n",
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.120425). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/87006 [===================>..........] - ETA: 2s - loss: 0.0419"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.130463). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61200/87006 [====================>.........] - ETA: 3s - loss: 0.0419"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.109220). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 16s 184us/step - loss: 0.0417\n",
      "Epoch 9/100000\n",
      "87006/87006 [==============================] - 19s 218us/step - loss: 0.041320s - loss: 0. - ETA: 20 - ETA: 17s  - ETA: - ETA: 7s -  - ETA: 6s - loss: 0 - ETA:   - ETA: 0s - loss: 0\n",
      "Epoch 10/100000\n",
      "87006/87006 [==============================] - 6s 69us/step - loss: 0.0391\n",
      "Epoch 11/100000\n",
      "87006/87006 [==============================] - 20s 230us/step - loss: 0.0385\n",
      "Epoch 12/100000\n",
      "87006/87006 [==============================] - 24s 277us/step - loss: 0.0379A: 1s - loss: 0 - ETA: 1s - loss: \n",
      "Epoch 13/100000\n",
      "  900/87006 [..............................] - ETA: 26s - loss: 0.0381"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.131560). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1800/87006 [..............................] - ETA: 35s - loss: 0.0378"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.104546). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n",
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.118551). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 14s 163us/step - loss: 0.0375\n",
      "Epoch 14/100000\n",
      "87006/87006 [==============================] - 9s 104us/step - loss: 0.0372 1s\n",
      "Epoch 15/100000\n",
      "87006/87006 [==============================] - 7s 78us/step - loss: 0.0373\n",
      "Epoch 16/100000\n",
      "87006/87006 [==============================] - 12s 140us/step - loss: 0.0362\n",
      "Epoch 17/100000\n",
      "87006/87006 [==============================] - 11s 121us/step - loss: 0.0352 ETA: 9s  - ETA: \n",
      "Epoch 18/100000\n",
      "87006/87006 [==============================] - 15s 169us/step - loss: 0.03013s -  - ETA: 2s - loss:  - ETA\n",
      "Epoch 19/100000\n",
      "87006/87006 [==============================] - 14s 156us/step - loss: 0.0170TA: 8s - loss: 0.025 - ETA: 8s - loss: - ET\n",
      "Epoch 20/100000\n",
      "87006/87006 [==============================] - 11s 128us/step - loss: 0.00420s - loss: 0.0\n",
      "Epoch 21/100000\n",
      "87006/87006 [==============================] - 12s 133us/step - loss: 0.0027\n",
      "Epoch 22/100000\n",
      "87006/87006 [==============================] - 13s 154us/step - loss: 0.0025 E - ETA:  - ETA: 0s - loss: 0.0\n",
      "Epoch 23/100000\n",
      "87006/87006 [==============================] - 15s 169us/step - loss: 0.0029\n",
      "Epoch 24/100000\n",
      "87006/87006 [==============================] - 12s 141us/step - loss: 0.0016\n",
      "Epoch 25/100000\n",
      "87006/87006 [==============================] - 14s 166us/step - loss: 0.0024\n",
      "Epoch 26/100000\n",
      "87006/87006 [==============================] - 14s 162us/step - loss: 0.00145s - l\n",
      "Epoch 27/100000\n",
      "87006/87006 [==============================] - 10s 121us/step - loss: 0.0013\n",
      "Epoch 28/100000\n",
      "87006/87006 [==============================] - 9s 101us/step - loss: 0.0013\n",
      "Epoch 29/100000\n",
      "87006/87006 [==============================] - 6s 66us/step - loss: 0.0012\n",
      "Epoch 30/100000\n",
      "87006/87006 [==============================] - 11s 131us/step - loss: 0.0012\n",
      "Epoch 31/100000\n",
      "87006/87006 [==============================] - 8s 92us/step - loss: 0.0011: - ETA: 1s - loss: 0.00 - ETA: 1s - lo - ETA: 0s - loss: 0.00 - ETA: 0s - loss: \n",
      "Epoch 32/100000\n",
      "87006/87006 [==============================] - 15s 170us/step - loss: 7.1449e-045s - loss: 5.4385e- - ETA: 1s - l\n",
      "Epoch 33/100000\n",
      "87006/87006 [==============================] - 15s 173us/step - loss: 4.7201e-042s - loss: 4.7589e - ETA: \n",
      "Epoch 34/100000\n",
      "87006/87006 [==============================] - 10s 119us/step - loss: 9.1986e-04 loss: 0.0 - ETA: 1s - loss - ETA: 1s -  - ETA: 0s - loss: 9.234\n",
      "Epoch 35/100000\n",
      "87006/87006 [==============================] - 14s 159us/step - loss: 5.6433e-044s - - ETA: 4s - loss: 5\n",
      "Epoch 36/100000\n",
      "87006/87006 [==============================] - 7s 77us/step - loss: 8.8742e-04\n",
      "Epoch 37/100000\n",
      "87006/87006 [==============================] - 14s 155us/step - loss: 4.4241e-04\n",
      "Epoch 38/100000\n",
      "87006/87006 [==============================] - 8s 87us/step - loss: 0.0015\n",
      "Epoch 39/100000\n",
      "87006/87006 [==============================] - 13s 149us/step - loss: 5.0602e-04- ETA: 6s - l - ETA: 3s - loss:  - ETA: 3s - ETA\n",
      "Epoch 40/100000\n",
      "87006/87006 [==============================] - 11s 132us/step - loss: 5.0266e-04\n",
      "Epoch 41/100000\n",
      "87006/87006 [==============================] - 7s 85us/step - loss: 0.0089\n",
      "Epoch 42/100000\n",
      "87006/87006 [==============================] - 11s 125us/step - loss: 0.00551s - lo\n",
      "Epoch 43/100000\n",
      "87006/87006 [==============================] - 15s 173us/step - loss: 0.0047\n",
      "Epoch 44/100000\n",
      "87006/87006 [==============================] - 16s 182us/step - loss: 0.0037\n",
      "Epoch 45/100000\n",
      "87006/87006 [==============================] - 11s 130us/step - loss: 0.0031\n",
      "Epoch 46/100000\n",
      "87006/87006 [==============================] - 17s 198us/step - loss: 0.00220s - loss:\n",
      "Epoch 47/100000\n",
      "87006/87006 [==============================] - 8s 89us/step - loss: 0.0020\n",
      "Epoch 48/100000\n",
      " 1200/87006 [..............................] - ETA: 39s - loss: 0.0012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.106291). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 14s 165us/step - loss: 0.00135s -  - ETA: 4s - l - ETA: 4 - ETA: 3s  - ETA: 1s - loss: 0 - ETA: \n",
      "Epoch 49/100000\n",
      "  900/87006 [..............................] - ETA: 23s - loss: 0.0040"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.132552). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 6s 72us/step - loss: 0.0013\n",
      "Epoch 50/100000\n",
      "87006/87006 [==============================] - 7s 85us/step - loss: 0.0011 ETA: 0s - loss: 0.00\n",
      "Epoch 51/100000\n",
      "87006/87006 [==============================] - 6s 64us/step - loss: 9.1541e-04\n",
      "Epoch 52/100000\n",
      "87006/87006 [==============================] - 11s 129us/step - loss: 0.00303s - loss - ETA: 3s - l - ETA: 3s - loss: 0 - ETA: 2s - l\n",
      "Epoch 53/100000\n",
      "  600/87006 [..............................] - ETA: 21s - loss: 0.0012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.121390). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n",
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.110487). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 14s 165us/step - loss: 0.0013 ETA: 0s - loss: 0.001\n",
      "Epoch 54/100000\n",
      "87006/87006 [==============================] - 10s 118us/step - loss: 6.4220e-04\n",
      "Epoch 55/100000\n",
      "87006/87006 [==============================] - 11s 125us/step - loss: 7.1141e-040s - loss: \n",
      "Epoch 56/100000\n",
      " 7800/87006 [=>............................] - ETA: 23s - loss: 6.5052e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.104343). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 11s 122us/step - loss: 6.5802e-04\n",
      "Epoch 57/100000\n",
      "87006/87006 [==============================] - 11s 125us/step - loss: 5.8011e-042s - loss: 4.5479e - ETA: 2s - loss: 4.49 - ETA:\n",
      "Epoch 58/100000\n",
      "87006/87006 [==============================] - 8s 89us/step - loss: 0.0012: 9s - loss - ETA:\n",
      "Epoch 59/100000\n",
      "87006/87006 [==============================] - 10s 114us/step - loss: 5.0915e-040\n",
      "Epoch 60/100000\n",
      "87006/87006 [==============================] - 11s 130us/step - loss: 5.4722e-04\n",
      "Epoch 61/100000\n",
      "87006/87006 [==============================] - 9s 107us/step - loss: 4.6552e-04\n",
      "Epoch 62/100000\n",
      "87006/87006 [==============================] - 10s 111us/step - loss: 5.4190e-04- E\n",
      "Epoch 63/100000\n",
      "87006/87006 [==============================] - 6s 72us/step - loss: 5.1234e-04: 4s - lo - ETA: 2s - l - ETA: 0s - loss: 4.34\n",
      "Epoch 64/100000\n",
      "87006/87006 [==============================] - 10s 111us/step - loss: 7.0432e-04\n",
      "Epoch 65/100000\n",
      "87006/87006 [==============================] - 8s 89us/step - loss: 4.0760e-04: 1s - l - ETA: 1s - lo - ETA: 0s - lo - ETA: 0s - los - ETA: 0s - loss: 4.0755e-0\n",
      "Epoch 66/100000\n",
      "87006/87006 [==============================] - 13s 146us/step - loss: 6.8290e-04TA - ETA: 3s - los - ETA: 3s - loss: 7.5389e - ETA: 3s - loss: 7.5099e- - ETA: 2s - lo - ETA: 1s - loss: 6.8942e-0 - ETA: 1s - loss:\n",
      "Epoch 67/100000\n",
      "87006/87006 [==============================] - 10s 120us/step - loss: 5.4112e-042s\n",
      "Epoch 68/100000\n",
      " 1200/87006 [..............................] - ETA: 22s - loss: 2.7793e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.162782). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n",
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.104836). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 7s 82us/step - loss: 4.0314e-04\n",
      "Epoch 69/100000\n",
      "87006/87006 [==============================] - 4s 42us/step - loss: 6.4188e-04\n",
      "Epoch 70/100000\n",
      "87006/87006 [==============================] - 4s 49us/step - loss: 4.6858e-04\n",
      "Epoch 71/100000\n",
      "87006/87006 [==============================] - 4s 41us/step - loss: 3.4738e-04\n",
      "Epoch 72/100000\n",
      "87006/87006 [==============================] - 5s 59us/step - loss: 5.1441e-04\n",
      "Epoch 73/100000\n",
      "87006/87006 [==============================] - 4s 49us/step - loss: 4.7704e-04\n",
      "Epoch 74/100000\n",
      "87006/87006 [==============================] - 7s 84us/step - loss: 5.6626e-04\n",
      "Epoch 75/100000\n",
      "87006/87006 [==============================] - 7s 77us/step - loss: 3.7360e-04:  - ETA: 5s - loss: 3.02\n",
      "Epoch 76/100000\n",
      "87006/87006 [==============================] - 8s 97us/step - loss: 3.7452e-04: 1s - loss: 3.0760e - E - ETA:  - ETA: 0s - lo - ETA: 0s - loss: 3.7540e-\n",
      "Epoch 77/100000\n",
      "79200/87006 [==========================>...] - ETA: 0s - loss: 4.4645e-04  ETA: 5s - loss: 1.5138e - ETA: 10s - loss - ETA: 7s - loss: 4. - ETA: 5s - loss: 4.2375 - ETA: 5s - loss: - ETA: 5s - loss: 3. - ETA: 4s - loss: 3 - ETA: 3s - "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.107986). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 12s 137us/step - loss: 4.2048e-04\n",
      "Epoch 78/100000\n",
      "87006/87006 [==============================] - 11s 126us/step - loss: 3.7430e-04\n",
      "Epoch 79/100000\n",
      "87006/87006 [==============================] - 8s 97us/step - loss: 5.2397e-04\n",
      "Epoch 80/100000\n",
      "87006/87006 [==============================] - 11s 125us/step - loss: 4.0213e-04\n",
      "Epoch 81/100000\n",
      "87006/87006 [==============================] - 14s 162us/step - loss: 3.5433e-041s - loss: 3.8 - ETA: 0s - loss: \n",
      "Epoch 82/100000\n",
      "  900/87006 [..............................] - ETA: 22s - loss: 8.9690e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.134154). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1500/87006 [..............................] - ETA: 34s - loss: 1.1607e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.105458). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 13s 153us/step - loss: 4.1836e-044\n",
      "Epoch 83/100000\n",
      "87006/87006 [==============================] - 9s 98us/step - loss: 3.8217e-04\n",
      "Epoch 84/100000\n",
      "87006/87006 [==============================] - 5s 63us/step - loss: 4.4099e-04: 0s - loss: 4.\n",
      "Epoch 85/100000\n",
      "87006/87006 [==============================] - 13s 152us/step - loss: 4.0905e-04ETA: 8s - loss: 1.6576e - ETA: 8s - loss: 1 - ETA: - ETA: 6s -  - ETA: 2s - - ETA: 0s - loss: 4\n",
      "Epoch 86/100000\n",
      "87006/87006 [==============================] - 12s 133us/step - loss: 4.9428e-049s - loss: 2. - ETA: 9s - loss: 2.0761e - ETA: 9s - ETA: 0s - loss: 4.9606e-\n",
      "Epoch 87/100000\n",
      "87006/87006 [==============================] - 10s 120us/step - loss: 2.4214e-042s - loss: 2.6727e-0 - ETA: 2\n",
      "Epoch 88/100000\n",
      "87006/87006 [==============================] - 9s 107us/step - loss: 4.0772e-04 4s - los - ETA: 2s - loss: 4.87 - ETA: 2s - lo\n",
      "Epoch 89/100000\n",
      "87006/87006 [==============================] - 5s 62us/step - loss: 4.8028e-04: 0s - loss:\n",
      "Epoch 90/100000\n",
      "87006/87006 [==============================] - 6s 73us/step - loss: 3.8997e-04\n",
      "Epoch 91/100000\n",
      "87006/87006 [==============================] - 4s 41us/step - loss: 3.3182e-04\n",
      "Epoch 92/100000\n",
      "87006/87006 [==============================] - 4s 46us/step - loss: 4.2866e-04\n",
      "Epoch 93/100000\n",
      "87006/87006 [==============================] - 4s 46us/step - loss: 2.4926e-04\n",
      "Epoch 94/100000\n",
      "87006/87006 [==============================] - 5s 59us/step - loss: 4.1306e-04\n",
      "Epoch 95/100000\n",
      "87006/87006 [==============================] - 9s 99us/step - loss: 4.1423e-04: 2s - loss: 3.\n",
      "Epoch 96/100000\n",
      "87006/87006 [==============================] - 12s 139us/step - loss: 5.4537e-04 ETA: 0s - loss: 5\n",
      "Epoch 97/100000\n",
      "87006/87006 [==============================] - 9s 101us/step - loss: 2.7545e-04\n",
      "Epoch 98/100000\n",
      "87006/87006 [==============================] - 10s 118us/step - loss: 6.4436e-043s - loss: 5 - ETA: 5s - l - E - ETA: 1s -\n",
      "Epoch 99/100000\n",
      "87006/87006 [==============================] - 7s 76us/step - loss: 1.7147e-04: 1s - ETA: 0s - loss: \n",
      "Epoch 100/100000\n",
      "87006/87006 [==============================] - 10s 110us/step - loss: 3.2011e-04 - val_loss: 8.0124e-05\n",
      "Epoch 101/100000\n",
      "87006/87006 [==============================] - 10s 115us/step - loss: 3.3093e-042s - - ETA: 2s - \n",
      "Epoch 102/100000\n",
      "87006/87006 [==============================] - 9s 100us/step - loss: 3.5224e-04 1s - loss: 3.9224e-0 - ETA: 1s - loss:  - ETA: 1s - l - ETA: 0s - loss\n",
      "Epoch 103/100000\n",
      "87006/87006 [==============================] - 5s 55us/step - loss: 2.7065e-04\n",
      "Epoch 104/100000\n",
      "87006/87006 [==============================] - 12s 139us/step - loss: 4.9007e-041s - loss: 5. - ETA: 0s - loss: 5.167 - ETA: 0s - loss: 5.0838e- - ETA: 0s - loss: 5. - ETA: 0s - loss: 4.9414e\n",
      "Epoch 105/100000\n",
      "87006/87006 [==============================] - 10s 111us/step - loss: 2.1286e-0411s - loss - ETA - ETA: 10s \n",
      "Epoch 106/100000\n",
      "87006/87006 [==============================] - 9s 109us/step - loss: 3.5412e-04\n",
      "Epoch 107/100000\n",
      "87006/87006 [==============================] - 6s 66us/step - loss: 2.7261e-04\n",
      "Epoch 108/100000\n",
      "87006/87006 [==============================] - 6s 72us/step - loss: 3.5684e-04s  - ETA: 5s - loss:  - E\n",
      "Epoch 109/100000\n",
      "87006/87006 [==============================] - 5s 57us/step - loss: 4.1044e-04: 0s - loss: 4.2209\n",
      "Epoch 110/100000\n",
      "87006/87006 [==============================] - 7s 81us/step - loss: 2.2722e-04\n",
      "Epoch 111/100000\n",
      "87006/87006 [==============================] - 10s 119us/step - loss: 3.3431e-048s - loss:  - ETA: 7s - loss: 3.\n",
      "Epoch 112/100000\n",
      "87006/87006 [==============================] - 4s 48us/step - loss: 4.1759e-04\n",
      "Epoch 113/100000\n",
      "87006/87006 [==============================] - 11s 128us/step - loss: 2.4342e-041s - loss: 2. - ETA: 0s - loss: 2.5\n",
      "Epoch 114/100000\n",
      "  900/87006 [..............................] - ETA: 34s - loss: 7.9534e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.152267). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n",
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.139073). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3300/87006 [>.............................] - ETA: 18s - loss: 6.3547e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.107221). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 12s 136us/step - loss: 3.4738e-040s - loss: 3.615 - ETA: 0s - loss: 3.\n",
      "Epoch 115/100000\n",
      "87006/87006 [==============================] - 11s 130us/step - loss: 2.1702e-045 - ETA: 4s - loss: 2.8189 - ETA: 3s - loss:  - ETA: 2s\n",
      "Epoch 116/100000\n",
      "87006/87006 [==============================] - 12s 138us/step - loss: 3.2703e-048s - loss: 2.7800e - ETA: 8s - loss: 2.7 - ETA: 0s - loss\n",
      "Epoch 117/100000\n",
      "87006/87006 [==============================] - 12s 139us/step - loss: 2.8877e-04\n",
      "Epoch 118/100000\n",
      "87006/87006 [==============================] - 10s 112us/step - loss: 2.7274e-04 ETA: 11s -  - ETA: 14s - loss: 6. - ETA: 15s - loss: 6.9001e- - E - ETA: 0s - loss: 2.\n",
      "Epoch 119/100000\n",
      "87006/87006 [==============================] - 8s 94us/step - loss: 2.6903e-04\n",
      "Epoch 120/100000\n",
      "87006/87006 [==============================] - 9s 107us/step - loss: 3.4503e-04 4s - loss: 6.572 - ETA: 5s - loss:  - ETA: 5s - loss - ETA: 4s - lo - ETA: 5s -\n",
      "Epoch 121/100000\n",
      "87006/87006 [==============================] - 10s 117us/step - loss: 2.4719e-04\n",
      "Epoch 122/100000\n",
      "87006/87006 [==============================] - 6s 69us/step - loss: 2.6709e-04\n",
      "Epoch 123/100000\n",
      " 3900/87006 [>.............................] - ETA: 5s - loss: 7.7110e-0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.100287). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 10s 114us/step - loss: 2.6228e-04\n",
      "Epoch 124/100000\n",
      "87006/87006 [==============================] - 11s 126us/step - loss: 3.3858e-04\n",
      "Epoch 125/100000\n",
      "87006/87006 [==============================] - 13s 150us/step - loss: 3.6294e-04ETA: 1s\n",
      "Epoch 126/100000\n",
      "87006/87006 [==============================] - 8s 94us/step - loss: 2.1316e-04\n",
      "Epoch 127/100000\n",
      "87006/87006 [==============================] - 5s 53us/step - loss: 2.0305e-04: 0s - loss: 2.04\n",
      "Epoch 128/100000\n",
      "87006/87006 [==============================] - 10s 109us/step - loss: 3.2659e-044s - lo  - - ETA: 0s - loss: 3.2783e-\n",
      "Epoch 129/100000\n",
      "87006/87006 [==============================] - 5s 57us/step - loss: 2.3252e-04: 5s - los - ETA: 0s - loss: 2.\n",
      "Epoch 130/100000\n",
      "87006/87006 [==============================] - 12s 136us/step - loss: 2.1947e-044 - ETA\n",
      "Epoch 131/100000\n",
      "87006/87006 [==============================] - 9s 102us/step - loss: 2.8605e-04\n",
      "Epoch 132/100000\n",
      "87006/87006 [==============================] - 8s 88us/step - loss: 3.0159e-04: 0s - loss: 3.0333 - ETA: 0s - loss: 3.04 - ETA: 0s -\n",
      "Epoch 133/100000\n",
      "87006/87006 [==============================] - 8s 89us/step - loss: 2.1692e-04\n",
      "Epoch 134/100000\n",
      "87006/87006 [==============================] - 11s 123us/step - loss: 3.0061e-048s \n",
      "Epoch 135/100000\n",
      "87006/87006 [==============================] - 11s 121us/step - loss: 1.8459e-040s - l\n",
      "Epoch 136/100000\n",
      "87006/87006 [==============================] - 10s 119us/step - loss: 2.7357e-044s -  - ETA: 3s - loss: 2.2114e -  - ET\n",
      "Epoch 137/100000\n",
      "87006/87006 [==============================] - 6s 65us/step - loss: 2.4046e-04\n",
      "Epoch 138/100000\n",
      "87006/87006 [==============================] - 11s 126us/step - loss: 2.3365e-045s - loss: 2 - ETA: 4s - loss: 2 - ETA: 4s - loss: 3.1062e - ETA: 3s \n",
      "Epoch 139/100000\n",
      "87006/87006 [==============================] - ETA: 0s - loss: 2.4834e-04- ETA: 1s - ETA: 0s - loss:  - ETA: 0s - loss - 5s 59us/step - loss: 2.4859e-04\n",
      "Epoch 140/100000\n",
      "87006/87006 [==============================] - 9s 108us/step - loss: 2.4467e-04 2s - loss: 2. - ETA:  - ETA: 0s \n",
      "Epoch 141/100000\n",
      "87006/87006 [==============================] - 7s 76us/step - loss: 2.5972e-04\n",
      "Epoch 142/100000\n",
      "87006/87006 [==============================] - 6s 68us/step - loss: 2.3792e-04\n",
      "Epoch 143/100000\n",
      "87006/87006 [==============================] - 8s 86us/step - loss: 2.0126e-04\n",
      "Epoch 144/100000\n",
      "87006/87006 [==============================] - 7s 85us/step - loss: 3.1435e-04\n",
      "Epoch 145/100000\n",
      "87006/87006 [==============================] - 6s 70us/step - loss: 2.1418e-04\n",
      "Epoch 146/100000\n",
      "87006/87006 [==============================] - 5s 62us/step - loss: 1.9378e-04\n",
      "Epoch 147/100000\n",
      "87006/87006 [==============================] - 13s 146us/step - loss: 2.5779e-04\n",
      "Epoch 148/100000\n",
      "87006/87006 [==============================] - 11s 129us/step - loss: 1.4482e-0412s - ETA: - ET - ETA: 1s - loss: 1.5632 - ETA: 1s - lo - ETA: 1s \n",
      "Epoch 149/100000\n",
      "  900/87006 [..............................] - ETA: 27s - loss: 6.7494e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.119875). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 13s 153us/step - loss: 2.6952e-042s - loss: 2.751 - ET\n",
      "Epoch 150/100000\n",
      "87006/87006 [==============================] - 15s 167us/step - loss: 2.2592e-041s - los\n",
      "Epoch 151/100000\n",
      " 1200/87006 [..............................] - ETA: 23s - loss: 5.6752e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.130975). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 12s 141us/step - loss: 1.7101e-04\n",
      "Epoch 152/100000\n",
      "87006/87006 [==============================] - 10s 114us/step - loss: 2.3707e-04\n",
      "Epoch 153/100000\n",
      "87006/87006 [==============================] - 11s 132us/step - loss: 2.5507e-04\n",
      "Epoch 154/100000\n",
      "87006/87006 [==============================] - 10s 113us/step - loss: 2.3412e-043s \n",
      "Epoch 155/100000\n",
      "87006/87006 [==============================] - 14s 165us/step - loss: 3.1285e-04\n",
      "Epoch 156/100000\n",
      "87006/87006 [==============================] - 8s 95us/step - loss: 2.0733e-04\n",
      "Epoch 157/100000\n",
      "87006/87006 [==============================] - 11s 129us/step - loss: 1.2237e-041s - loss: 1.1947e-0 - ETA: 1s  - ETA: 0s - loss: 1.\n",
      "Epoch 158/100000\n",
      "87006/87006 [==============================] - 11s 128us/step - loss: 1.8311e-0410s - loss:  - ETA: 9 - ETA: 9 - \n",
      "Epoch 159/100000\n",
      "87006/87006 [==============================] - 6s 66us/step - loss: 2.6886e-04\n",
      "Epoch 160/100000\n",
      "87006/87006 [==============================] - 12s 137us/step - loss: 2.4027e-043s - l - ETA\n",
      "Epoch 161/100000\n",
      "87006/87006 [==============================] - 8s 90us/step - loss: 1.6868e-04: 2s  - ETA: 0s - loss: 1.6991e\n",
      "Epoch 162/100000\n",
      "87006/87006 [==============================] - 11s 123us/step - loss: 2.0692e-046s - loss: 1.39 - ETA: 6s - loss: 1.41 - ETA: 1\n",
      "Epoch 163/100000\n",
      "87006/87006 [==============================] - 14s 160us/step - loss: 2.0067e-04\n",
      "Epoch 164/100000\n",
      "87006/87006 [==============================] - 11s 122us/step - loss: 1.6856e-047s  - ETA: 5s - loss: 1. - ETA: 3s - loss: 1.5181e - E - ETA\n",
      "Epoch 165/100000\n",
      "87006/87006 [==============================] - 11s 122us/step - loss: 2.3259e-04\n",
      "Epoch 166/100000\n",
      "87006/87006 [==============================] - 13s 144us/step - loss: 1.7557e-041\n",
      "Epoch 167/100000\n",
      "87006/87006 [==============================] - 11s 122us/step - loss: 2.0669e-04 0s - loss: 2.0664e-0\n",
      "Epoch 168/100000\n",
      "87006/87006 [==============================] - 13s 149us/step - loss: 3.4462e-04 - ETA: 0s - l\n",
      "Epoch 169/100000\n",
      "87006/87006 [==============================] - 14s 158us/step - loss: 1.1205e-041s\n",
      "Epoch 170/100000\n",
      "87006/87006 [==============================] - 13s 152us/step - loss: 1.7164e-04A - E - ETA\n",
      "Epoch 171/100000\n",
      "87006/87006 [==============================] - 11s 132us/step - loss: 1.9107e-046s - loss: 1.9 - ETA: 6s - l - ETA: 2s - loss: 2.1916e-0 - E\n",
      "Epoch 172/100000\n",
      "87006/87006 [==============================] - 13s 150us/step - loss: 2.0543e-048s  - ETA: 5s - loss: 1.8429e- - ETA: 5s - loss: 1.9000 - ETA: 4s - loss: 1.7079e - ETA: 4s - lo - E - ETA: 1s - loss: 2.2 - ETA: 0s - lo\n",
      "Epoch 173/100000\n",
      "87006/87006 [==============================] - 7s 85us/step - loss: 1.8731e-04\n",
      "Epoch 174/100000\n",
      "87006/87006 [==============================] - 11s 127us/step - loss: 1.7019e-04\n",
      "Epoch 175/100000\n",
      "87006/87006 [==============================] - 10s 115us/step - loss: 1.5998e-043s -\n",
      "Epoch 176/100000\n",
      "87006/87006 [==============================] - 14s 165us/step - loss: 1.7574e-048s - loss: 3.9887e- - ETA: 9s - loss: 3.9063e-0 - ETA: 9s - loss: 3 - ETA: 8s - loss:  - ETA: 5s - loss: - ETA: 4s - loss: 2.298 - ETA: 2s - loss - ET\n",
      "Epoch 177/100000\n",
      " 6900/87006 [=>............................] - ETA: 17s - loss: 3.4527e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.101283). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 13s 155us/step - loss: 1.7813e-043s - loss: - ETA: 0s - loss: 1.\n",
      "Epoch 178/100000\n",
      "87006/87006 [==============================] - 10s 116us/step - loss: 1.6986e-043s  - ETA: 2s - loss: 1.7 - ETA: 2 - ETA - ETA: 0s - loss: 1.691\n",
      "Epoch 179/100000\n",
      " 1500/87006 [..............................] - ETA: 14s - loss: 7.4689e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.108283). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 13s 145us/step - loss: 2.2476e-04 - ETA: 3s - loss: 2. - ETA: 3s - lo - E - ETA: 2s - los - ETA: 1s - loss: 2 - ETA: 1\n",
      "Epoch 180/100000\n",
      "87006/87006 [==============================] - 12s 137us/step - loss: 1.7697e-046s  - ETA: 6s - loss: 1.0197e-0 - ETA: \n",
      "Epoch 181/100000\n",
      "87006/87006 [==============================] - 9s 100us/step - loss: 1.2794e-04\n",
      "Epoch 182/100000\n",
      "87006/87006 [==============================] - 6s 64us/step - loss: 1.6175e-04: 0s - loss: 1.6213e-\n",
      "Epoch 183/100000\n",
      "87006/87006 [==============================] - 13s 145us/step - loss: 1.4626e-040s - loss: 1.4820e - ETA: 0s - loss: 1.4832\n",
      "Epoch 184/100000\n",
      "87006/87006 [==============================] - 10s 120us/step - loss: 1.8265e-04\n",
      "Epoch 185/100000\n",
      "59400/87006 [===================>..........] - ETA: 5s - loss: 4.5254e-04- ETA: 8s - loss: 1.9492 - ET"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.103043). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60300/87006 [===================>..........] - ETA: 5s - loss: 4.6107e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.102258). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61800/87006 [====================>.........] - ETA: 5s - loss: 4.6746e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.102737). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n",
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.101361). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 17s 193us/step - loss: 3.6901e-044s\n",
      "Epoch 186/100000\n",
      "87006/87006 [==============================] - 10s 113us/step - loss: 1.4695e-04\n",
      "Epoch 187/100000\n",
      "87006/87006 [==============================] - 8s 96us/step - loss: 1.5401e-04\n",
      "Epoch 188/100000\n",
      "87006/87006 [==============================] - 11s 130us/step - loss: 2.4061e-04\n",
      "Epoch 189/100000\n",
      "87006/87006 [==============================] - 13s 150us/step - loss: 1.1634e-04\n",
      "Epoch 190/100000\n",
      "87006/87006 [==============================] - 13s 151us/step - loss: 1.5672e-044s - loss: 2.2404 - E - \n",
      "Epoch 191/100000\n",
      "87006/87006 [==============================] - 13s 146us/step - loss: 1.6672e-045s - l - ETA: \n",
      "Epoch 192/100000\n",
      "87006/87006 [==============================] - 6s 69us/step - loss: 1.3181e-04\n",
      "Epoch 193/100000\n",
      "87006/87006 [==============================] - 11s 127us/step - loss: 1.4474e-046s -  - ETA: 1s -\n",
      "Epoch 194/100000\n",
      "87006/87006 [==============================] - 6s 74us/step - loss: 1.7602e-04: 1s - los\n",
      "Epoch 195/100000\n",
      "87006/87006 [==============================] - 9s 101us/step - loss: 1.2856e-04 6s - los - ETA: - \n",
      "Epoch 196/100000\n",
      "87006/87006 [==============================] - 6s 71us/step - loss: 1.5696e-04\n",
      "Epoch 197/100000\n",
      "87006/87006 [==============================] - 9s 107us/step - loss: 1.2857e-04\n",
      "Epoch 198/100000\n",
      "87006/87006 [==============================] - 5s 60us/step - loss: 1.4282e-04\n",
      "Epoch 199/100000\n",
      "87006/87006 [==============================] - 7s 84us/step - loss: 1.3421e-04A:\n",
      "Epoch 200/100000\n",
      "87006/87006 [==============================] - 9s 105us/step - loss: 1.3206e-04 - val_loss: 1.4793e-043e- - ETA: 1s -  -\n",
      "Epoch 201/100000\n",
      "87006/87006 [==============================] - 11s 130us/step - loss: 1.3398e-044 - ETA - ETA: 3s - loss - ETA: 2s - loss:  - ETA: 2s - los - ETA: \n",
      "Epoch 202/100000\n",
      "87006/87006 [==============================] - 14s 158us/step - loss: 1.2425e-042s - loss: 1. - ETA: 1s - loss: 1.38 - \n",
      "Epoch 203/100000\n",
      "87006/87006 [==============================] - 9s 108us/step - loss: 1.3263e-04  - ETA: 7s - loss: 1.55 - ETA:\n",
      "Epoch 204/100000\n",
      "87006/87006 [==============================] - 9s 99us/step - loss: 1.3231e-04\n",
      "Epoch 205/100000\n",
      "87006/87006 [==============================] - 7s 77us/step - loss: 1.3580e-04\n",
      "Epoch 206/100000\n",
      "87006/87006 [==============================] - 5s 59us/step - loss: 1.3478e-04\n",
      "Epoch 207/100000\n",
      "87006/87006 [==============================] - 9s 106us/step - loss: 1.2774e-04 1s - - ETA: 1s - loss:\n",
      "Epoch 208/100000\n",
      "87006/87006 [==============================] - 9s 98us/step - loss: 1.3941e-04\n",
      "Epoch 209/100000\n",
      "87006/87006 [==============================] - 10s 120us/step - loss: 1.1784e-041s - loss: 1.24 - ETA: 0s - loss: 1.2312 - ETA: 0s - loss: 1.2310e- - ETA: 0s - los\n",
      "Epoch 210/100000\n",
      "87006/87006 [==============================] - 11s 125us/step - loss: 1.1818e-044s - loss: 9.4696e - ETA: 4s - loss: 9.5604e- - ETA: 4s -  - ETA: 3s - ETA: 2s - loss: 1 - ETA - ETA: 0s - loss: 1.2\n",
      "Epoch 211/100000\n",
      "87006/87006 [==============================] - 12s 138us/step - loss: 1.2363e-045s - loss: 1.2923e - ET - ETA: 0s - loss: 1.2494e- - ETA: 0s - loss: 1.2340e\n",
      "Epoch 212/100000\n",
      "87006/87006 [==============================] - 12s 136us/step - loss: 1.1823e-04\n",
      "Epoch 213/100000\n",
      "87006/87006 [==============================] - 9s 106us/step - loss: 1.7822e-04\n",
      "Epoch 214/100000\n",
      "87006/87006 [==============================] - 5s 59us/step - loss: 1.1745e-04: 2s - loss: 7.  - ETA: 0s - loss: 1.\n",
      "Epoch 215/100000\n",
      "87006/87006 [==============================] - 10s 115us/step - loss: 9.8641e-05 ETA: 4s - loss: 6.8 - ETA: 3s  - ETA: 2s - - ETA\n",
      "Epoch 216/100000\n",
      "87006/87006 [==============================] - 6s 72us/step - loss: 1.1519e-04:\n",
      "Epoch 217/100000\n",
      "87006/87006 [==============================] - 8s 97us/step - loss: 1.2823e-04ETA:\n",
      "Epoch 218/100000\n",
      "87006/87006 [==============================] - 11s 129us/step - loss: 1.2214e-04\n",
      "Epoch 219/100000\n",
      "87006/87006 [==============================] - 5s 57us/step - loss: 9.4381e-05: 2s - l\n",
      "Epoch 220/100000\n",
      "87006/87006 [==============================] - 6s 68us/step - loss: 1.0808e-04: 1s -  - ETA: 1s - lo\n",
      "Epoch 221/100000\n",
      "87006/87006 [==============================] - 8s 93us/step - loss: 1.2706e-04: 2s - loss: 1.2 - ETA: 1s - lo\n",
      "Epoch 222/100000\n",
      "87006/87006 [==============================] - 6s 70us/step - loss: 1.2779e-04\n",
      "Epoch 223/100000\n",
      "87006/87006 [==============================] - 8s 91us/step - loss: 9.6870e-05: 0s - los\n",
      "Epoch 224/100000\n",
      "87006/87006 [==============================] - 8s 92us/step - loss: 1.8907e-04: 0s - loss: 1.9362e\n",
      "Epoch 225/100000\n",
      "87006/87006 [==============================] - 10s 118us/step - loss: 8.2686e-05 5s - loss: 5.997 - ETA: 5s - loss: 9.0293\n",
      "Epoch 226/100000\n",
      "87006/87006 [==============================] - 13s 149us/step - loss: 1.0364e-040s - loss: 1.0379\n",
      "Epoch 227/100000\n",
      "87006/87006 [==============================] - 7s 78us/step - loss: 8.8386e-05\n",
      "Epoch 228/100000\n",
      "87006/87006 [==============================] - 10s 120us/step - loss: 1.0846e-04\n",
      "Epoch 229/100000\n",
      "87006/87006 [==============================] - 9s 101us/step - loss: 1.1195e-04 7s - loss\n",
      "Epoch 230/100000\n",
      "87006/87006 [==============================] - 8s 89us/step - loss: 1.1801e-04: 2s - loss:  \n",
      "Epoch 231/100000\n",
      "87006/87006 [==============================] - 8s 90us/step - loss: 8.6799e-05\n",
      "Epoch 232/100000\n",
      "87006/87006 [==============================] - 9s 102us/step - loss: 8.2059e-05\n",
      "Epoch 233/100000\n",
      "87006/87006 [==============================] - 8s 89us/step - loss: 1.2639e-04: 4s\n",
      "Epoch 234/100000\n",
      "87006/87006 [==============================] - 7s 86us/step - loss: 8.7514e-05\n",
      "Epoch 235/100000\n",
      "87006/87006 [==============================] - 9s 103us/step - loss: 9.7785e-05\n",
      "Epoch 236/100000\n",
      "87006/87006 [==============================] - 12s 141us/step - loss: 1.3225e-04- ETA: 0s - loss: 1.3223e-0\n",
      "Epoch 237/100000\n",
      " 3000/87006 [>.............................] - ETA: 12s - loss: 3.6157e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.168297). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 10s 115us/step - loss: 6.5940e-0513s - lo - ETA: 14s - loss: 2. - ETA: 12s  - ETA: 1\n",
      "Epoch 238/100000\n",
      "87006/87006 [==============================] - 15s 167us/step - loss: 1.2962e-045s - loss: 9.2979e-0 - ETA - ETA: 4s -  - ETA: 2s \n",
      "Epoch 239/100000\n",
      "87006/87006 [==============================] - 8s 95us/step - loss: 5.6907e-05: 2s - loss: - ETA: 0s - loss: 5.6375\n",
      "Epoch 240/100000\n",
      "87006/87006 [==============================] - 7s 79us/step - loss: 9.0246e-05\n",
      "Epoch 241/100000\n",
      "87006/87006 [==============================] - 11s 123us/step - loss: 8.1631e-05\n",
      "Epoch 242/100000\n",
      "87006/87006 [==============================] - ETA: 0s - loss: 8.3588e-0 - 12s 137us/step - loss: 8.3588e-05\n",
      "Epoch 243/100000\n",
      "87006/87006 [==============================] - 11s 121us/step - loss: 8.0347e-05 - ETA: 0s - lo - ETA: 0s - loss: 8.0342e-0\n",
      "Epoch 244/100000\n",
      "87006/87006 [==============================] - 10s 119us/step - loss: 9.1653e-05\n",
      "Epoch 245/100000\n",
      "87006/87006 [==============================] - 10s 113us/step - loss: 9.1242e-051s - l - ETA\n",
      "Epoch 246/100000\n",
      "87006/87006 [==============================] - 12s 140us/step - loss: 7.2193e-05\n",
      "Epoch 247/100000\n",
      "87006/87006 [==============================] - 12s 134us/step - loss: 1.0107e-04 ETA: 0s - loss: 1.0070e-\n",
      "Epoch 248/100000\n",
      "87006/87006 [==============================] - 12s 143us/step - loss: 8.2248e-050s - loss: 8.36 - ETA: 0s - loss: 8.379\n",
      "Epoch 249/100000\n",
      "87006/87006 [==============================] - 14s 166us/step - loss: 6.9844e-056s -  - ETA - ETA: 3s - loss: 5.25 - ETA: 3s - loss:  - ETA: 3s - lo - ETA: 2s -  - ETA: 1s - los\n",
      "Epoch 250/100000\n",
      "87006/87006 [==============================] - 12s 141us/step - loss: 9.7597e-05TA: 1s - loss:\n",
      "Epoch 251/100000\n",
      "87006/87006 [==============================] - 14s 159us/step - loss: 6.1134e-050s - loss: - ETA: 0s - loss: 5.8802\n",
      "Epoch 252/100000\n",
      "87006/87006 [==============================] - 9s 107us/step - loss: 7.9583e-05\n",
      "Epoch 253/100000\n",
      "87006/87006 [==============================] - 14s 159us/step - loss: 9.4357e-053s - loss: 1.1 -  - ETA: 1s  - ETA: 0s - loss: 9.5460e-0 - ETA: 0s - loss: 9.5211\n",
      "Epoch 254/100000\n",
      "87006/87006 [==============================] - 5s 62us/step - loss: 5.6445e-05\n",
      "Epoch 255/100000\n",
      "87006/87006 [==============================] - 9s 100us/step - loss: 6.8970e-05 2s - loss: 6.60 - ETA: 1s - loss: 6.72 - ETA: 1s - loss: 6.4218e - ETA: 1s - loss: 6.3830e- - ETA: 1s - loss: 6.574\n",
      "Epoch 256/100000\n",
      "87006/87006 [==============================] - 11s 131us/step - loss: 7.7034e-05TA: - ETA: 2s - loss: 7.0 -\n",
      "Epoch 257/100000\n",
      "87006/87006 [==============================] - 12s 139us/step - loss: 6.6337e-053s - loss: 6. - ETA: 3\n",
      "Epoch 258/100000\n",
      "87006/87006 [==============================] - 9s 99us/step - loss: 7.2828e-05\n",
      "Epoch 259/100000\n",
      "87006/87006 [==============================] - 10s 120us/step - loss: 8.1925e-053s - loss:  - ETA: 3s - - ETA: 2s - loss: 7 - ETA: 0s - loss: 8.6451 - ETA: 0s - loss: 8.\n",
      "Epoch 260/100000\n",
      "87006/87006 [==============================] - 9s 105us/step - loss: 5.0954e-05\n",
      "Epoch 261/100000\n",
      "87006/87006 [==============================] - 12s 140us/step - loss: 6.9610e-053s - loss: 8 - ETA: 2\n",
      "Epoch 262/100000\n",
      "87006/87006 [==============================] - 11s 123us/step - loss: 8.8061e-05\n",
      "Epoch 263/100000\n",
      "87006/87006 [==============================] - 11s 125us/step - loss: 5.2878e-052s\n",
      "Epoch 264/100000\n",
      "87006/87006 [==============================] - 7s 83us/step - loss: 7.5536e-05: 2s - loss: 6. - ETA: 1s - loss:  - ETA: 1s - ETA: 1s\n",
      "Epoch 265/100000\n",
      "87006/87006 [==============================] - 9s 101us/step - loss: 6.4599e-05: 1\n",
      "Epoch 266/100000\n",
      "87006/87006 [==============================] - 14s 157us/step - loss: 6.2594e-053s - loss: 7 - ETA: 1\n",
      "Epoch 267/100000\n",
      "87006/87006 [==============================] - 14s 163us/step - loss: 6.2798e-05 ETA: 16s - loss - ETA: 12s  - ETA: 6s -  - ETA: 1s - - ETA: 0s - loss: 6.3\n",
      "Epoch 268/100000\n",
      "87006/87006 [==============================] - 15s 177us/step - loss: 7.8460e-0510 - ETA: 1s - loss: 6.090 - ETA: 1s - loss: - ETA: 0s - loss: 7.\n",
      "Epoch 269/100000\n",
      "87006/87006 [==============================] - 10s 110us/step - loss: 4.6360e-052s - loss: 4.934 - ETA:\n",
      "Epoch 270/100000\n",
      "87006/87006 [==============================] - 12s 133us/step - loss: 6.8205e-05\n",
      "Epoch 271/100000\n",
      "87006/87006 [==============================] - 10s 111us/step - loss: 6.1870e-05\n",
      "Epoch 272/100000\n",
      "87006/87006 [==============================] - 9s 102us/step - loss: 6.5038e-05 2s -  - ETA: 1s - loss: 6. - ETA: 0s - loss:  - ETA: 0s - loss: 6.2\n",
      "Epoch 273/100000\n",
      "87006/87006 [==============================] - 14s 163us/step - loss: 5.9300e-05- E\n",
      "Epoch 274/100000\n",
      "87006/87006 [==============================] - 17s 196us/step - loss: 6.3984e-05 ETA: 3s - loss: 7.129 - ETA: 1s - loss:\n",
      "Epoch 275/100000\n",
      " 2700/87006 [..............................] - ETA: 17s - loss: 1.3349e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.100278). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 15s 167us/step - loss: 7.0233e-0511s - ETA: 9s - loss:  - ETA: 9s - loss: - ETA: 8s - loss: 6.5847e-0 - ETA: 8s - loss: - ETA: 7s - loss: 6. - ETA: 7s - loss: 7. - ET - ETA - ETA: 1s - loss: 7.312 - ETA: 1s - loss: 7. - ETA: 0s - loss: 7 - ETA: 0s - loss:\n",
      "Epoch 276/100000\n",
      "87006/87006 [==============================] - 10s 119us/step - loss: 6.4565e-05\n",
      "Epoch 277/100000\n",
      "87006/87006 [==============================] - 13s 152us/step - loss: 4.5718e-05\n",
      "Epoch 278/100000\n",
      " 1200/87006 [..............................] - ETA: 33s - loss: 5.1261e-05 ETA: 21s - loss: 1.7498"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.150663). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 14s 158us/step - loss: 6.6588e-052s\n",
      "Epoch 279/100000\n",
      "87006/87006 [==============================] - 10s 115us/step - loss: 6.3743e-05\n",
      "Epoch 280/100000\n",
      "87006/87006 [==============================] - 5s 62us/step - loss: 4.1575e-05: \n",
      "Epoch 281/100000\n",
      "87006/87006 [==============================] - 10s 115us/step - loss: 5.6275e-053s - loss: 7.03 - ETA: 3s - loss: - ETA: 1s - loss: 5.9\n",
      "Epoch 282/100000\n",
      "87006/87006 [==============================] - ETA: 0s - loss: 5.0893e-05- ETA: 2s - loss: 5.62  - 11s 130us/step - loss: 5.1083e-05\n",
      "Epoch 283/100000\n",
      "87006/87006 [==============================] - 7s 86us/step - loss: 8.1000e-05: 0s - loss: 8. - ETA: 0s - loss:\n",
      "Epoch 284/100000\n",
      "87006/87006 [==============================] - 8s 95us/step - loss: 4.2973e-05\n",
      "Epoch 285/100000\n",
      "87006/87006 [==============================] - 6s 66us/step - loss: 6.6539e-05\n",
      "Epoch 286/100000\n",
      "87006/87006 [==============================] - 6s 70us/step - loss: 6.2377e-05\n",
      "Epoch 287/100000\n",
      "87006/87006 [==============================] - 7s 84us/step - loss: 4.7451e-05: 1s - los\n",
      "Epoch 288/100000\n",
      "87006/87006 [==============================] - 8s 95us/step - loss: 5.4288e-05: 1s  - ETA: 1s \n",
      "Epoch 289/100000\n",
      "87006/87006 [==============================] - 8s 91us/step - loss: 4.3674e-05\n",
      "Epoch 290/100000\n",
      "87006/87006 [==============================] - 7s 76us/step - loss: 6.8553e-05ETA: 0s - loss: 6.9640\n",
      "Epoch 291/100000\n",
      " 1200/87006 [..............................] - ETA: 19s - loss: 2.9060e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.109283). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 9s 101us/step - loss: 3.5660e-05 2s - loss -  - ETA: 0s - loss: 3.6\n",
      "Epoch 292/100000\n",
      "87006/87006 [==============================] - 10s 119us/step - loss: 5.9706e-051s - loss: \n",
      "Epoch 293/100000\n",
      "87006/87006 [==============================] - 11s 126us/step - loss: 4.8462e-058s - loss: 5.1779e-05 - ETA: 9s - ETA: 9s - loss: 3.0367e-0 - ETA: 9s - loss: 3.0003e - ETA: 9s - loss: -  - ETA: 6s - loss: 3.8507 - ETA: 6s - loss: -  - ETA: 3 - E\n",
      "Epoch 294/100000\n",
      "87006/87006 [==============================] - 7s 78us/step - loss: 5.1622e-05\n",
      "Epoch 295/100000\n",
      "87006/87006 [==============================] - 8s 96us/step - loss: 4.3953e-05: 0s - loss: 4.5365 - ETA: 0s - loss: 4.5178 - ETA: 0s - loss: \n",
      "Epoch 296/100000\n",
      "87006/87006 [==============================] - 9s 103us/step - loss: 4.8591e-05- \n",
      "Epoch 297/100000\n",
      "87006/87006 [==============================] - 10s 115us/step - loss: 5.4893e-053s  - ETA: 3s - los - ETA: 3s - ETA: 0s - loss: \n",
      "Epoch 298/100000\n",
      "87006/87006 [==============================] - 12s 136us/step - loss: 5.1539e-058s  - ETA: 6s - loss: 3. - ETA: 5s - ETA: 0s - loss: 5.1703\n",
      "Epoch 299/100000\n",
      "87006/87006 [==============================] - 10s 118us/step - loss: 3.7227e-052s - - ETA: 1s - loss: 3. - ETA: 0s - \n",
      "Epoch 300/100000\n",
      "87006/87006 [==============================] - 11s 126us/step - loss: 4.7614e-05 - val_loss: 5.7276e-06ETA: 1s - loss: 4.5 - ETA: 1s - loss: 4.53 - ETA: 0s - loss: 4.4200e - ETA: 0s - loss: 4.389 - ETA: 0s - loss: 4.7163\n",
      "Epoch 301/100000\n",
      "  600/87006 [..............................] - ETA: 28s - loss: 9.9142e-06"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.181824). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n",
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.153424). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3000/87006 [>.............................] - ETA: 14s - loss: 1.5270e-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.125023). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 9s 101us/step - loss: 5.0384e-05 9 - \n",
      "Epoch 302/100000\n",
      "87006/87006 [==============================] - 8s 94us/step - loss: 3.9762e-05: 1\n",
      "Epoch 303/100000\n",
      "87006/87006 [==============================] - 11s 130us/step - loss: 5.2802e-05\n",
      "Epoch 304/100000\n",
      "87006/87006 [==============================] - 8s 90us/step - loss: 4.2583e-05\n",
      "Epoch 305/100000\n",
      "87006/87006 [==============================] - 11s 124us/step - loss: 3.7890e-058s - loss: 3.9060e-0 - ETA: 8s - loss - ETA: 0s - lo\n",
      "Epoch 306/100000\n",
      "87006/87006 [==============================] - 10s 118us/step - loss: 4.5124e-05\n",
      "Epoch 307/100000\n",
      "87006/87006 [==============================] - 12s 133us/step - loss: 5.0943e-055s - los - ETA: 5s - loss: 6.32 - ETA: 4s - loss: 6.0 - ETA: 4s - loss: 5.7891 - ETA: 4s - loss\n",
      "Epoch 308/100000\n",
      "87006/87006 [==============================] - 9s 100us/step - loss: 3.9734e-05 1s - loss: 3 - ETA: 1s - loss: 3.8930 - ETA: 1s -\n",
      "Epoch 309/100000\n",
      "87006/87006 [==============================] - 9s 108us/step - loss: 4.9229e-05\n",
      "Epoch 310/100000\n",
      "87006/87006 [==============================] - 6s 70us/step - loss: 3.6000e-05: 1s - los\n",
      "Epoch 311/100000\n",
      "87006/87006 [==============================] - 12s 138us/step - loss: 4.7892e-05\n",
      "Epoch 312/100000\n",
      "87006/87006 [==============================] - 7s 86us/step - loss: 5.4861e-05\n",
      "Epoch 313/100000\n",
      "87006/87006 [==============================] - 10s 113us/step - loss: 3.4289e-05\n",
      "Epoch 314/100000\n",
      "87006/87006 [==============================] - 11s 124us/step - loss: 3.9644e-05\n",
      "Epoch 315/100000\n",
      "87006/87006 [==============================] - 7s 80us/step - loss: 6.0513e-05\n",
      "Epoch 316/100000\n",
      "87006/87006 [==============================] - 7s 78us/step - loss: 3.1030e-05\n",
      "Epoch 317/100000\n",
      "87006/87006 [==============================] - 9s 98us/step - loss: 4.0248e-05\n",
      "Epoch 318/100000\n",
      "87006/87006 [==============================] - 12s 144us/step - loss: 6.9633e-05\n",
      "Epoch 319/100000\n",
      "87006/87006 [==============================] - 6s 64us/step - loss: 1.3306e-05: \n",
      "Epoch 320/100000\n",
      "87006/87006 [==============================] - 13s 146us/step - loss: 3.9169e-052s - loss: 4.45\n",
      "Epoch 321/100000\n",
      "87006/87006 [==============================] - 6s 74us/step - loss: 4.5482e-05\n",
      "Epoch 322/100000\n",
      " 1200/87006 [..............................] - ETA: 29s - loss: 3.5700e-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.100279). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87006/87006 [==============================] - 11s 122us/step - loss: 4.3749e-05TA: 0s - loss: 4\n",
      "Epoch 323/100000\n",
      "87006/87006 [==============================] - 14s 155us/step - loss: 3.5027e-050s - loss: 3.5606e-0 - ETA: 0s - loss: 3.55\n",
      "Epoch 324/100000\n",
      "87006/87006 [==============================] - 13s 154us/step - loss: 4.2460e-05\n",
      "Epoch 325/100000\n",
      "87006/87006 [==============================] - 14s 156us/step - loss: 4.7299e-054s - lo -\n",
      "Epoch 326/100000\n",
      "87006/87006 [==============================] - 11s 121us/step - loss: 2.8219e-05 5s - loss:  - ETA: 6s - loss:  - ETA: 5s - loss: - ETA - ETA: 1s - loss:  - ETA: 0s - loss:  - ETA: 0s - loss: 2.8\n",
      "Epoch 327/100000\n",
      "87006/87006 [==============================] - 12s 143us/step - loss: 4.6238e-05\n",
      "Epoch 328/100000\n",
      "87006/87006 [==============================] - 8s 97us/step - loss: 6.2157e-05\n",
      "Epoch 329/100000\n",
      "87006/87006 [==============================] - 13s 152us/step - loss: 2.1266e-058s  - - ETA: 6s - loss: 1.06 - ETA: 5s - loss: 1.0475e- - ETA: 5s - los - ET - ETA: 1s - - ETA: 0s - loss: 2\n",
      "Epoch 330/100000\n",
      "87006/87006 [==============================] - 9s 101us/step - loss: 3.8384e-05 1s - l - ETA: 0s - loss: \n",
      "Epoch 331/100000\n",
      "87006/87006 [==============================] - 14s 156us/step - loss: 4.6271e-050s - loss: 4.71\n",
      "Epoch 332/100000\n",
      "87006/87006 [==============================] - 11s 128us/step - loss: 3.3708e-05\n",
      "Epoch 333/100000\n",
      "87006/87006 [==============================] - 12s 137us/step - loss: 3.6781e-050s - loss: 3\n",
      "Epoch 334/100000\n",
      "87006/87006 [==============================] - 9s 100us/step - loss: 4.8480e-05 0s - loss: 3.\n",
      "Epoch 335/100000\n",
      "87006/87006 [==============================] - 12s 138us/step - loss: 2.7506e-054s -  - ETA: 4s - ETA: 3s - loss: 2.4332e- - ETA: 3s - - ETA: 1s - loss: 2.9708e- - ETA: 1s - loss: 2.9519e- - ETA\n",
      "Epoch 336/100000\n",
      "87006/87006 [==============================] - 16s 182us/step - loss: 3.9265e-05 5s - loss\n",
      "Epoch 337/100000\n",
      "42600/87006 [=============>................] - ETA: 6s - loss: 9.3799e-0"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-cf1751244df2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0minput_data_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_data_all\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;32m~\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    199\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mbatch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \"\"\"\n\u001b[0;32m    365\u001b[0m         \u001b[1;31m# For backwards compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[1;31m# will be handled by on_epoch_end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\theano_v1\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[0mprev_total_width\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_width\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dynamic_display\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                 \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\b'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mprev_total_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m                 \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\theano_v1\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[1;31m# newlines imply flush in subprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\theano_v1\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\theano_v1\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    398\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    399\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\theano_v1\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(\n",
    "    input_data_all, output_data_all,\n",
    "    batch_size=300, epochs=100000,\n",
    "    verbose=1, validation_split=0.1,validation_freq=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rsquared: 99.95008688561789\n"
     ]
    }
   ],
   "source": [
    "### Now we load the test data and predict the model out of that\n",
    "### What we do next is calculate R-squared and display it.\n",
    "test_x = np.load('test_sample.npy')#,dtype=theano.config.floatX)\n",
    "test_y= np.load('test_target.npy')#,dtype=theano.config.floatX)\n",
    "test_x = np.load('test_sample.npy')#,dtype=theano.config.floatX)\n",
    "test_y= np.load('test_target.npy')#,dtype=theano.config.floatX)\n",
    "sample_out= model.predict(test_x)\n",
    "ss_tot = np.sum((test_y-np.mean(test_y))**2)\n",
    "SS_res = np.sum((sample_out.reshape(-1)-test_y)**2)\n",
    "print(\"Rsquared:\",(1 - SS_res/ss_tot)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared of 99.95 is not great, but also not bad for first training. We can still improve this by retraining, just like we did it when we used lasagne package!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.505761 ],\n",
       "       [1.5068358],\n",
       "       [1.5069517],\n",
       "       ...,\n",
       "       [1.2960402],\n",
       "       [1.2948159],\n",
       "       [1.294792 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### And we save the model;\n",
    "\n",
    "model.save(\"model.h1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
